from bs4 import BeautifulSoup
import urllib.request
import ssl
import requests
import json
import csv
context = ssl._create_unverified_context()
# print('context', context)

def get_new_feed():
    url = 'https://ncov.moh.gov.vn/web/guest/trang-chu'
    # page = urllib.request.urlopen(url, context=context)
    page = requests.get(url, verify=False)
    soup = BeautifulSoup(page.text, 'html.parser')
    new_feed = soup.find(class_='journal-content-article')
    print(new_feed)
    return new_feed
# get_new_feed()
def to_soup(html):
    return BeautifulSoup(html, 'html.parser')
def get_journey():
    url = 'https://ncov.moh.gov.vn/dong-thoi-gian'
    page = requests.get(url, verify=False)
    soup = to_soup(page.text)
    journey = soup.find_all("div",class_='timeline')[:5]

    print(len(journey))
    print(journey[0])
    dt_arr= []
    cnt_arr = []

    for j in journey:
        print(j.find("h3").text)
        dt_arr.append(j.find("h3").text)
        print(j.find("p").text)
        cnt_arr.append(j.find("p").text)


    return dt_arr,cnt_arr
# get_journey()
def get_hotline(all=False):
    if all:
        url = 'https://ncov.moh.gov.vn/documents/20182/6848000/Duongdaynong/'
    else:
        url = 'ÄÆ°á»ng dÃ¢y nÃ³ng: 19009095 / 19003228'
    return url
# get_hotline(True)
def get_video(video_id=0):
    video_list = {
        # Khuyen cao chung
        0:"https://ncov.moh.gov.vn/documents/20182/6863405/video003/",
        # Dieu khien phuong tien
        1:"https://ncov.moh.gov.vn/documents/20182/6863405/video002/",
        # Cach ly tai nha
        2:"https://ncov.moh.gov.vn/documents/20182/6863405/c%C3%A1ch+ly+01/",
        # Nhugn ai can cach ly
        3:"https://ncov.moh.gov.vn/documents/20182/6863405/C%C3%A1ch+ly+t%E1%BA%A1i+nh%C3%A0/",
        # Huong dan cach ly
        4:"https://ncov.moh.gov.vn/documents/20182/6863405/H%C6%B0%E1%BB%9Bng+d%E1%BA%ABn+c%C3%A1ch+ly+tai+nh%C3%A0%281%29/"
    }
    return video_list[video_id]
def get_symptom():
    symptom_txt = """"""
    return symptom_txt
def get_summary():
    url = 'https://suckhoetoandan.vn/'
    page = requests.get(url, verify=False)
    soup = BeautifulSoup(page.text, 'html.parser')

    main_row = soup.find_all("div",class_ ="box-heading")[1:]


    # print(main_row)

    all_of_it = "Sá» LIá»†U LÅ¨Y Káº¾ Äáº¾N HIá»†N Táº I:\n"
    all_of_it += "ToÃ n cáº§u:\n"

    number = main_row[0].find_all("span",class_ = "box-total")
    all_of_it += "Sá»‘ ngÆ°á»i bá»‹ nhiá»…m: " + number[0].text + "\n"
    all_of_it += "Sá»‘ ngÆ°á»i tá»­ vong: " + number[2].text+ "\n"
    all_of_it += "Sá»‘ ngÆ°á»i bÃ¬nh phá»¥c: " + number[4].text+ "\n"

    all_of_it += "Viá»‡t Nam:\n"

    number = main_row[0].find_all("span", class_="box-total")
    all_of_it += "Sá»‘ ngÆ°á»i bá»‹ nhiá»…m: " + number[1].text + "\n"
    all_of_it += "Sá»‘ ngÆ°á»i tá»­ vong: " + number[3].text + "\n"
    all_of_it += "Sá»‘ ngÆ°á»i bÃ¬nh phá»¥c: " + number[5].text + "\n"

    all_of_it += "Sá» LÆ¯á»¢NG TÄ‚NG TRONG NGÃ€Y:\n"
    all_of_it += "ToÃ n cáº§u:\n"

    number = main_row[1].find_all("span", class_="box-total")
    all_of_it += "Sá»‘ ngÆ°á»i bá»‹ nhiá»…m: " + number[0].text + "\n"
    all_of_it += "Sá»‘ ngÆ°á»i tá»­ vong: " + number[2].text + "\n"
    all_of_it += "Sá»‘ ngÆ°á»i bÃ¬nh phá»¥c: " + number[4].text + "\n"

    all_of_it += "Viá»‡t Nam:\n"

    number = main_row[1].find_all("span", class_="box-total")
    all_of_it += "Sá»‘ ngÆ°á»i bá»‹ nhiá»…m: " + number[1].text + "\n"
    all_of_it += "Sá»‘ ngÆ°á»i tá»­ vong: " + number[3].text + "\n"
    all_of_it += "Sá»‘ ngÆ°á»i bÃ¬nh phá»¥c: " + number[5].text + "\n"

    print(all_of_it)
def sort_by_year(d):
    '''
    helper function for sorting a list of dictionaries'''
    return d.get('ma', None)
# get_summary()
def get_detail_domestic():
    url = 'https://ncov.moh.gov.vn/web/guest/trang-chu'
    page = requests.get(url, verify=False)

    with open('crawler/data/info_citylist.txt', mode='r') as infile:
        reader = csv.reader(infile)
        mydict = {rows[0]: rows[1] for rows in reader}
        # print(reader)
    # print(mydict)
    # print(page.text)
    #28856
    #65976
    key = "function getInfoByMa(ma)"
    html = page.text
    start_domestic = html.index(key) + len(key)
    html = html[start_domestic:]
    key = "_congbothongke_WAR_coronadvcportlet_jsonData : '"
    start_domestic = html.index(key) + len(key)
    key = "}]'"
    end_domestic = html.index(key, start_domestic) + len(key) - 1;

    json_str = html[start_domestic:end_domestic]

    json_obj = json.loads(json_str)

    print(json_obj)

    all_of_it = "ğŸ›‘CHI TIáº¾T TÃŒNH HÃŒNH COVID-19 TRONG NÆ¯á»šC"
    for row in sorted(json_obj, key=sort_by_year,reverse=True):
            if row['ma']!='' and row['ma']!='--Chá»n Ä‘á»‹a phÆ°Æ¡ng--' and row['soCaNhiem']!='0':
                all_of_it += ("\n%s - Nhiá»…m: %s - Tá»­ vong: %s - Nghi nhiá»…m: %s - BÃ¬nh phá»¥c: %s" % (mydict[row['ma']].strip(), row['soCaNhiem'],row['tuVong'],row['nghiNhiem'],row['binhPhuc']))  # test

    print(all_of_it)
# get_detail_domestic()
def get_news():
    url = 'https://suckhoetoandan.vn/'
    page = requests.get(url, verify=False)
    soup = BeautifulSoup(page.text, 'html.parser')

    main_row = soup.find("div",class_ ="list-new-left-type3").find_all("div", class_ ="item-new")

    message_str = """{
        "attachment": {
            "type": "template",
            "payload": {
                "template_type": "generic",
                "elements": [
                    [CONTENT]
                ]
            }
        }
    }"""

    inside_cnt_template = """{
                        "title": "[TITLE]",
                        "image_url": "[IMG]",
                        "subtitle": "[SUBTITLE]",
                        "default_action": {
                            "type": "web_url",
                            "url": "[URL]",
                            "webview_height_ratio": "full"
                        },
                        "buttons": [
                            {
                                "type": "web_url",
                                "url": "[URL]",
                                "title": "Xem tin ngay"
                            }
                        ]
                    },"""
    inside_cnt =""
    for row in main_row:
        tmp = inside_cnt_template.replace("[TITLE]",row.find('a').get('title'))
        tmp = tmp.replace("[URL]",row.find('a').get('href'))
        if row.find('img').get('src')[:4] == "http":
            tmp = tmp.replace("[IMG]",row.find('img').get('src'))
        else:
            tmp = tmp.replace("[IMG]", url+row.find('img').get('src'))

        tmp = tmp.replace("[SUBTITLE]",row.find('a').get('title'))
        inside_cnt += tmp

    message_str = message_str.replace("[CONTENT]",inside_cnt[:-1])

    message = json.loads(message_str)

    print(message)

def get_new_source():
    url = 'https://beta.dantri.com.vn/suc-khoe/nua-dem-bo-y-te-cong-bo-cung-luc-9-ca-mac-covid-19-moi-20200319211303006.htm'
    page = requests.get(url, verify=False)
    soup = BeautifulSoup(page.text, 'html.parser')

    main_row = soup.find("div", class_="dantri-widget dantri-widget--corona")

    print(main_row)
# get_new_source()
        # text = 'ğŸ›‘TRIá»†U CHá»¨NG NHIá»„M CORONA QUA Tá»ªNG NGÃ€Y'+'\n'+\
        #         'ğŸ”¸NgÃ y 1 ~ NgÃ y 3'+'\n'+\
        #             'â–ª Triá»‡u chá»©ng giá»‘ng bá»‡nh cáº£m'+'\n'+\
        #             'â–ª ViÃªm há»ng nháº¹, hÆ¡i Ä‘au'+'\n'+\
        #             'â–ª KhÃ´ng nÃ³ng sá»‘t. KhÃ´ng má»‡t má»i. Váº«n Äƒn uá»‘ng bÃ¬nh thÆ°á»ng'+'\n'+\
        #         'ğŸ”¸NgÃ y 4'+'\n'+\
        #             'â–ª Cá»• há»ng Ä‘au nháº¹, ngÆ°á»i nÃ´n nao.'+'\n'+\
        #             'â–ª Báº¯t Ä‘áº§u khan tiáº¿ng.'+'\n'+\
        #             'â–ª Nhiá»‡t Ä‘á»™ cÆ¡ thá»ƒ dao Ä‘á»™ng 36.5~ (tuá»³ ngÆ°á»i)'+'\n'+\
        #             'â–ª Báº¯t Ä‘áº§u chÃ¡n Äƒn.'+'\n'+\
        #             'â–ª Äau Ä‘áº§u nháº¹'+'\n'+\
        #             'â–ª TiÃªu cháº£y nháº¹'+'\n'+\
        #         'ğŸ”¸NgÃ y 5'+'\n'+\
        #             'â–ª Äau há»ng, khan tiáº¿ng hÆ¡n'+'\n'+\
        #             'â–ª CÆ¡ thá»ƒ nÃ³ng nháº¹. Nhiá»‡t Ä‘á»™ tá»« 36.5~36.7'+'\n'+\
        #             'â–ª NgÆ°á»i má»‡t má»i, cáº£m tháº¥y Ä‘au khá»›p xÆ°Æ¡ng'+'\n'+\
        #             '** Giai Ä‘oáº¡n nÃ y khÃ³ nháº­n ra lÃ  cáº£m hay lÃ  nhiá»…m corona'+'\n'+\
        #         'ğŸ”¸NgÃ y 6'+'\n'+\
        #             'â–ª Báº¯t Ä‘áº§u sá»‘t nháº¹, khoáº£ng 37'+'\n'+\
        #             'â–ª Ho cÃ³ Ä‘Ã m hoáº·c ho khan'+'\n'+\
        #             'â–ª Äau há»ng khi Äƒn, nÃ³i hay nuá»‘t nÆ°á»›c bá»t'+'\n'+\
        #             'â–ª Má»‡t má»i, buá»“n nÃ´n'+'\n'+\
        #             'â–ª Thá»‰nh thoáº£ng khÃ³ khÄƒn trong viá»‡c hÃ­t thá»Ÿ'+'\n'+\
        #             'â–ª LÆ°ng, ngÃ³n tay Ä‘au lÃ¢m rÃ¢m'+'\n'+\
        #             'â–ª TiÃªu cháº£y, cÃ³ thá»ƒ nÃ´n Ã³i'+'\n'+\
        #         'ğŸ”¸NgÃ y 7'+'\n'+\
        #             'â–ª Sá»‘t cao hÆ¡n tá»« 37.4~37.8'+'\n'+\
        #             'â–ª Ho nhiá»u hÆ¡n, Ä‘Ã m nhiá»u hÆ¡n.'+'\n'+\
        #             'â–ª ToÃ n thÃ¢n Ä‘au nhá»©c. Äáº§u náº·ng nhÆ° Ä‘eo Ä‘Ã¡'+'\n'+\
        #             'â–ª Táº§n suáº¥t khÃ³ thá»Ÿ váº«n nhÆ° cÅ©.'+'\n'+\
        #             'â–ª TiÃªu cháº£y nhÃ¬u hÆ¡n'+'\n'+\
        #             'â–ª NÃ´n Ã³i'+'\n'+\
        #         'ğŸ”¸NgÃ y 8'+'\n'+\
        #             'â–ª Sá»‘t gáº§n má»©c 38 hoáº·c trÃªn 38'+'\n'+\
        #             'â–ª KhÃ³ thá»Ÿ hÆ¡n, má»—i khi hÃ­t thá»Ÿ cáº£m tháº¥y náº·ng lá»“ng ngá»±c. HÆ¡i thá»Ÿ khÃ² khÃ¨'+'\n'+\
        #             'â–ª Ho liÃªn tá»¥c, Ä‘Ã m nhiá»u, táº¯t tiáº¿ng'+'\n'+\
        #             'â–ª Äáº§u Ä‘au, khá»›p xÆ°Æ¡ng Ä‘au, lÆ°ng Ä‘au...'+'\n'+\
        #         'ğŸ”¸NgÃ y 9'+'\n'+\
        #             'â–ª CÃ¡c triá»‡u chá»©ng khÃ´ng thay Ä‘á»•i mÃ  trá»Ÿ nÃªn náº·ng hÆ¡n.'+'\n'+\
        #             'â–ª Sá»‘t tÄƒng giáº£m lá»™n xá»™n'+'\n'+\
        #             'â–ª Ho khÃ´ng bá»›t mÃ  náº·ng hÆ¡n trÆ°á»›c.'+'\n'+\
        #             'â–ª DÃ¹ cá»‘ gáº¯ng váº«n cáº£m tháº¥y khÃ³ hÃ­t thá»Ÿ.'+'\n'+\
        #             '** Táº¡i thá»i Ä‘iá»ƒm nÃ y, nÃªn Ä‘i xÃ©t nghiá»‡m mÃ¡u vÃ  chá»¥p XQuang phá»•i Ä‘á»ƒ kiá»ƒm tra'+'\n'+\
        #         'ğŸ›‘ChÃº Ã½:'+'\n'+\
        #         'â–ª ThÃ´ng tin Ä‘á»ƒ tham kháº£o.'+'\n'+\
        #         'â–ª Triá»‡u chá»©ng thay Ä‘á»•i tuá»³ theo sá»©c Ä‘á» khÃ¡ng cá»§a tá»«ng ngÆ°á»i. Ai khoáº» thÃ¬ máº¥t 10-14 ngÃ y má»›i phÃ¡t hiá»‡n. Ai khÃ´ng khoáº» thÃ¬ 4-5 ngÃ y'


# def get_summary():
#     url = 'https://suckhoetoandan.vn/'
#     page = requests.get(url, verify=False)
#     soup = BeautifulSoup(page.text, 'html.parser')

#     main_row = soup.find_all("div",class_ ="box-heading")[1:]


#     # print(main_row)

#     all_of_it = "ğŸ›‘ Sá» LIá»†U Äáº¾N HIá»†N Táº I:\n"
#     all_of_it += "ğŸŒ ToÃ n cáº§u:\n"

#     number = main_row[0].find_all("span",class_ = "box-total")
#     all_of_it += "Sá»‘ ngÆ°á»i bá»‹ nhiá»…m: " + number[0].text + "\n"
#     all_of_it += "Sá»‘ ngÆ°á»i tá»­ vong: " + number[2].text+ "\n"
#     all_of_it += "Sá»‘ ngÆ°á»i bÃ¬nh phá»¥c: " + number[4].text+ "\n"

#     all_of_it += "ğŸ‡»ğŸ‡³ Viá»‡t Nam:\n"

#     number = main_row[0].find_all("span", class_="box-total")
#     all_of_it += "Sá»‘ ngÆ°á»i bá»‹ nhiá»…m: " + number[1].text + "\n"
#     all_of_it += "Sá»‘ ngÆ°á»i tá»­ vong: " + number[3].text + "\n"
#     all_of_it += "Sá»‘ ngÆ°á»i bÃ¬nh phá»¥c: " + number[5].text + "\n"

#     all_of_it += "ğŸ›‘ Sá» LÆ¯á»¢NG TÄ‚NG TRONG NGÃ€Y:\n"
#     all_of_it += "ğŸŒ ToÃ n cáº§u:\n"

#     number = main_row[1].find_all("span", class_="box-total")
#     all_of_it += "Sá»‘ ngÆ°á»i bá»‹ nhiá»…m: " + number[0].text + "\n"
#     all_of_it += "Sá»‘ ngÆ°á»i tá»­ vong: " + number[2].text + "\n"
#     all_of_it += "Sá»‘ ngÆ°á»i bÃ¬nh phá»¥c: " + number[4].text + "\n"

#     all_of_it += "ğŸ‡»ğŸ‡³ Viá»‡t Nam:\n"

#     number = main_row[1].find_all("span", class_="box-total")
#     all_of_it += "Sá»‘ ngÆ°á»i bá»‹ nhiá»…m: " + number[1].text + "\n"
#     all_of_it += "Sá»‘ ngÆ°á»i tá»­ vong: " + number[3].text + "\n"
#     all_of_it += "Sá»‘ ngÆ°á»i bÃ¬nh phá»¥c: " + number[5].text + "\n"

#     # print(all_of_it)
#     return all_of_it
# def get_hotline():
#     url = 'https://ncov.moh.gov.vn/documents/20182/6848000/Duongdaynong/'
#     page = requests.get(url, verify=False)
#     soup = BeautifulSoup(page.text, 'html.parser')
#     img  = soup.find_all("a")
#     print(page)
#     # return img

# get_hotline()